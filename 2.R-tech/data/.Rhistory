# 1) root 150 100 setosa (0.33333333 0.33333333 0.33333333)
#   2) Petal.Length< 2.45 50   0 setosa (1.00000000 0.00000000 0.00000000) *
#   3) Petal.Length>=2.45 100  50 versicolor (0.00000000 0.50000000 0.50000000)
#     6) Petal.Width< 1.75 54   5 versicolor (0.00000000 0.90740741 0.09259259) *
#     7) Petal.Width>=1.75 46   1 virginica (0.00000000 0.02173913 0.97826087) *
##== 繪出決策樹
rpart.plot(iris.rpart)
##== 繪製決策樹
plot(iris.rpart);     text(iris.rpart)  #-- 6個葉結點, 其餘均為決策結點, 數據分裂為二元劃分
Type = predict(iris.rpart, newdata=X,level=0.95,interval="confidence")
##== 訓練模型
library(rpart);   library(rpart.plot)
iris.rpart = rpart(Species ~. , data=iris);   print(iris.rpart)
# 1) root 150 100 setosa (0.33333333 0.33333333 0.33333333)
#   2) Petal.Length< 2.45 50   0 setosa (1.00000000 0.00000000 0.00000000) *
#   3) Petal.Length>=2.45 100  50 versicolor (0.00000000 0.50000000 0.50000000)
#     6) Petal.Width< 1.75 54   5 versicolor (0.00000000 0.90740741 0.09259259) *
#     7) Petal.Width>=1.75 46   1 virginica (0.00000000 0.02173913 0.97826087) *
##== 繪出決策樹
rpart.plot(iris.rpart)
##== 訓練模型
library(rpart);   library(rpart.plot)
iris.rpart = rpart(Species ~. , data=iris);   print(iris.rpart)
##== 訓練模型
library(rpart);   library(rpart.plot)
iris.rpart = rpart(Type ~Rating + Reviews.log , data=XX);   print(iris.rpart)
iris.rpart = rpart(Type ~Rating + Reviews , data=XX);   print(iris.rpart)
iris.rpart = rpart(Type ~Rating , data=XX);   print(iris.rpart)
##== 訓練模型
library(rpart);   library(rpart.plot)
iris.rpart = rpart(Type ~ Rating + Reviews.log, data=XX,
control=rpart.control(minsplit=6, maxdepth=4));   print(iris.rpart)
# 1) root 150 100 setosa (0.33333333 0.33333333 0.33333333)
#   2) Petal.Length< 2.45 50   0 setosa (1.00000000 0.00000000 0.00000000) *
#   3) Petal.Length>=2.45 100  50 versicolor (0.00000000 0.50000000 0.50000000)
#     6) Petal.Width< 1.75 54   5 versicolor (0.00000000 0.90740741 0.09259259) *
#     7) Petal.Width>=1.75 46   1 virginica (0.00000000 0.02173913 0.97826087) *
##== 繪出決策樹
rpart.plot(iris.rpart)
#####=====*(2B) iris例說明決策樹tree操作程序及其中觀念 [殷,7.2, 7.9.2] =====#####
library(tree);library(rpart)
X$Reviews.log =log10(X$Reviews)
XX = as.data.frame(X[c(1:10,170:220),c("Rating","Reviews.log","Type")]);   dim(XX);   head(XX,2)
#因變數   #自變數
# iris.tree = tree(Type ~ Rating + Reviews.log, data=XX);    iris.tree
# iris.tree = tree(Type ~ Rating + Reviews, data=X[c(1:10,170:220),]);    iris.tree
# iris.rpart = rpart(Type ~ Rating + Reviews.log, data=XX);    iris.rpart
iris.rpart = rpart(Type ~ Rating + Reviews.log, data=XX,
control=rpart.control(minsplit=6, maxdepth=4));    iris.rpart
plot.app <- function(xx,yy,xxlab,yylab) {
plot(NULL,xlim=range(xx),ylim=range(yy),
main="classified scatter plot of iris data",
xlab=xxlab,ylab=yylab)
points(xx[Type=="Free"],yy[Type=="Free"],pch=1,col="blue")
points(xx[Type=="Paid"],yy[Type== "Paid"],pch=2,col="red")
legend("topleft",
legend=c("Free","Paid"), bty="n",
col=c("blue","red"),
x.intersp=0.5, y.intersp=0.5,
pch=c(1,2))
}
Reviews.log =log10(Reviews)
Rating.log = log10(Rating)
plot.app(Rating,Reviews.log, "Rating", "Reviews")  #收費不一定比較好
#####=====*(2B) iris例說明決策樹tree操作程序及其中觀念 [殷,7.2, 7.9.2] =====#####
library(tree);library(rpart)
X$Reviews.log =log10(X$Reviews)
XX = as.data.frame(X[c(1:10,170:220),c("Rating","Reviews.log","Type")]);   dim(XX);   head(XX,2)
#=========R-tech6================
setwd("/Users/juck30808/Documents/Github/USC_R_Git/2.R-tech/data")
#X <- as.data.frame(read_excel('/Users/juck30808/Documents/Github/USC_R_Git/2.R-tech/data/googleplaystore4_(7000).xlsX'))
X = read.csv("googleplaystore4_(7000).csv");   dim(X);   head(X,2)
##== App 收費 type 分布圖
dim(X); head(X,2)
attach(X)   ##== (1) 設定數據框為X, 可以精簡以下的變量表示
plot.app <- function(xx,yy,xxlab,yylab) {
plot(NULL,xlim=range(xx),ylim=range(yy),
main="classified scatter plot of iris data",
xlab=xxlab,ylab=yylab)
points(xx[Type=="Free"],yy[Type=="Free"],pch=1,col="blue")
points(xx[Type=="Paid"],yy[Type== "Paid"],pch=2,col="red")
legend("topleft",
legend=c("Free","Paid"), bty="n",
col=c("blue","red"),
x.intersp=0.5, y.intersp=0.5,
pch=c(1,2))
}
Reviews.log =log10(Reviews)
Rating.log = log10(Rating)
plot.app(Rating,Reviews.log, "Rating", "Reviews")  #收費不一定比較好
#####=====*(2B) iris例說明決策樹tree操作程序及其中觀念 [殷,7.2, 7.9.2] =====#####
library(tree);library(rpart)
X$Reviews.log =log10(X$Reviews)
XX = as.data.frame(X[c(1:10,170:220),c("Rating","Reviews.log","Type")]);   dim(XX);   head(XX,2)
#因變數   #自變數
# iris.tree = tree(Type ~ Rating + Reviews.log, data=XX);    iris.tree
# iris.tree = tree(Type ~ Rating + Reviews, data=X[c(1:10,170:220),]);    iris.tree
# iris.rpart = rpart(Type ~ Rating + Reviews.log, data=XX);    iris.rpart
iris.rpart = rpart(Type ~ Rating + Reviews.log, data=XX,
control=rpart.control(minsplit=6, maxdepth=4));    iris.rpart
##== 繪製決策樹
plot(iris.rpart);     text(iris.rpart)  #-- 6個葉結點, 其餘均為決策結點, 數據分裂為二元劃分
Type = predict(iris.rpart, newdata=X,level=0.95,interval="confidence")
##== 訓練模型
library(rpart);   library(rpart.plot)
iris.rpart = rpart(Type ~ Rating + Reviews.log, data=XX,
control=rpart.control(minsplit=6, maxdepth=4));   print(iris.rpart)
# 1) root 150 100 setosa (0.33333333 0.33333333 0.33333333)
#   2) Petal.Length< 2.45 50   0 setosa (1.00000000 0.00000000 0.00000000) *
#   3) Petal.Length>=2.45 100  50 versicolor (0.00000000 0.50000000 0.50000000)
#     6) Petal.Width< 1.75 54   5 versicolor (0.00000000 0.90740741 0.09259259) *
#     7) Petal.Width>=1.75 46   1 virginica (0.00000000 0.02173913 0.97826087) *
##== 繪出決策樹
rpart.plot(iris.rpart)
##== 預測值
Species.new.rpart3 = predict(iris.rpartA, newdata=iris);   Species.new.rpart3
iris.rpart = rpart(Type ~ Rating + Reviews.log, data=XX,
control=rpart.control(minsplit=16, maxdepth=4));   print(iris.rpart)
iris.rpart = rpart(Type ~ Rating + Reviews.log, data=XX,
control=rpart.control(minsplit=16, maxdepth=4));   print(iris.rpart)
#####=====*(2B) iris例說明決策樹tree操作程序及其中觀念 [殷,7.2, 7.9.2] =====#####
library(tree);library(rpart)
X$Reviews.log =log10(X$Reviews)
XX = as.data.frame(X[c(1:10,170:220),c("Rating","Reviews.log","Type")]);   dim(XX);   head(XX,2)
#因變數   #自變數
# iris.tree = tree(Type ~ Rating + Reviews.log, data=XX);    iris.tree
# iris.tree = tree(Type ~ Rating + Reviews, data=X[c(1:10,170:220),]);    iris.tree
# iris.rpart = rpart(Type ~ Rating + Reviews.log, data=XX);    iris.rpart
iris.rpart = rpart(Type ~ Rating + Reviews.log, data=XX,
control=rpart.control(minsplit=6, maxdepth=4));    iris.rpart
##== 繪製決策樹
plot(iris.rpart);     text(iris.rpart)  #-- 6個葉結點, 其餘均為決策結點, 數據分裂為二元劃分
Type = predict(iris.rpart, newdata=X,level=0.95,interval="confidence")
##== 訓練模型
library(rpart);   library(rpart.plot)
iris.rpart = rpart(Type ~ Rating + Reviews.log, data=XX,
control=rpart.control(minsplit=6, maxdepth=4));   print(iris.rpart)
# 1) root 150 100 setosa (0.33333333 0.33333333 0.33333333)
#   2) Petal.Length< 2.45 50   0 setosa (1.00000000 0.00000000 0.00000000) *
#   3) Petal.Length>=2.45 100  50 versicolor (0.00000000 0.50000000 0.50000000)
#     6) Petal.Width< 1.75 54   5 versicolor (0.00000000 0.90740741 0.09259259) *
#     7) Petal.Width>=1.75 46   1 virginica (0.00000000 0.02173913 0.97826087) *
##== 繪出決策樹
rpart.plot(iris.rpart)
##== 預測值
Species.new.rpart3 = predict(iris.rpartA, newdata=iris);   Species.new.rpart3
#####=====*(2B) iris例說明決策樹tree操作程序及其中觀念 [殷,7.2, 7.9.2] =====#####
library(tree);library(rpart)
X$Reviews.log =log10(X$Reviews)
XX = as.data.frame(X[c(1:10,170:220),c("Rating","Reviews.log","Type")]);   dim(XX);   head(XX,2)
#因變數   #自變數
# iris.tree = tree(Type ~ Rating + Reviews.log, data=XX);    iris.tree
# iris.tree = tree(Type ~ Rating + Reviews, data=X[c(1:10,170:220),]);    iris.tree
# iris.rpart = rpart(Type ~ Rating + Reviews.log, data=XX);    iris.rpart
iris.rpart = rpart(Type ~ Rating + Reviews.log, data=XX,
control=rpart.control(minsplit=6, maxdepth=4));    iris.rpart
##== 繪製決策樹
plot(iris.rpart);     text(iris.rpart)  #-- 6個葉結點, 其餘均為決策結點, 數據分裂為二元劃分
#=========R-tech6================
setwd("/Users/juck30808/Documents/Github/USC_R_Git/2.R-tech/data")
#X <- as.data.frame(read_excel('/Users/juck30808/Documents/Github/USC_R_Git/2.R-tech/data/googleplaystore4_(7000).xlsX'))
X = read.csv("googleplaystore4_(7000).csv");   dim(X);   head(X,2)
##== App 收費 type 分布圖
dim(X); head(X,2)
attach(X)   ##== (1) 設定數據框為X, 可以精簡以下的變量表示
plot.app <- function(xx,yy,xxlab,yylab) {
plot(NULL,xlim=range(xx),ylim=range(yy),
main="classified scatter plot of iris data",
xlab=xxlab,ylab=yylab)
points(xx[Type=="Free"],yy[Type=="Free"],pch=1,col="blue")
points(xx[Type=="Paid"],yy[Type== "Paid"],pch=2,col="red")
legend("topleft",
legend=c("Free","Paid"), bty="n",
col=c("blue","red"),
x.intersp=0.5, y.intersp=0.5,
pch=c(1,2))
}
Reviews.log =log10(Reviews)
Rating.log = log10(Rating)
plot.app(Rating,Reviews.log, "Rating", "Reviews")  #收費不一定比較好
#####=====*(2B) iris例說明決策樹tree操作程序及其中觀念 [殷,7.2, 7.9.2] =====#####
library(tree);library(rpart)
X$Reviews.log =log10(X$Reviews)
XX = as.data.frame(X[c(1:10,170:220),c("Rating","Reviews.log","Type")]);   dim(XX);   head(XX,2)
#因變數   #自變數
# iris.tree = tree(Type ~ Rating + Reviews.log, data=XX);    iris.tree
# iris.tree = tree(Type ~ Rating + Reviews, data=X[c(1:10,170:220),]);    iris.tree
# iris.rpart = rpart(Type ~ Rating + Reviews.log, data=XX);    iris.rpart
iris.rpart = rpart(Type ~ Rating + Reviews.log, data=XX,
control=rpart.control(minsplit=6, maxdepth=4));    iris.rpart
##== 繪製決策樹
plot(iris.rpart);     text(iris.rpart)  #-- 6個葉結點, 其餘均為決策結點, 數據分裂為二元劃分
Type = predict(iris.rpart, newdata=X,level=0.95,interval="confidence")
##== 訓練模型
library(rpart);   library(rpart.plot)
iris.rpart = rpart(Type ~ Rating + Reviews.log, data=XX,
control=rpart.control(minsplit=6, maxdepth=4));   print(iris.rpart)
rpart.plot(iris.rpart)
#####=====*(2B) iris例說明決策樹tree操作程序及其中觀念 [殷,7.2, 7.9.2] =====#####
library(tree)
##== 模型的訓練階段: 由輸入數據(u)輸出數據(y) 求取模型 M = tree(y~u)
iris.tree = tree(Species ~ .,data=iris);    iris.tree   #-- * denotes terminal node(葉結點或終結點)
#   3) Petal.Length > 2.45 100 138.600 versicolor ( 0.00000 0.50000 0.50000 )
#     6) Petal.Width < 1.75 54  33.320 versicolor ( 0.00000 0.90741 0.09259 )
#      12) Petal.Length < 4.95 48   9.721 versicolor ( 0.00000 0.97917 0.02083 )
#        24) Sepal.Length < 5.15 5   5.004 versicolor ( 0.00000 0.80000 0.20000 ) * 葉結點 FR24
#        25) Sepal.Length > 5.15 43   0.000 versicolor ( 0.00000 1.00000 0.00000 ) * 葉結點 R25
#      13) Petal.Length > 4.95 6   7.638 virginica ( 0.00000 0.33333 0.66667 ) * 葉結點 FR13
#     7) Petal.Width > 1.75 46   9.635 virginica ( 0.00000 0.02174 0.97826 )
#      14) Petal.Length < 4.95 6   5.407 virginica ( 0.00000 0.16667 0.83333 ) * 葉結點 FR14
#      15) Petal.Length > 4.95 40   0.000 virginica ( 0.00000 0.00000 1.00000 ) * 葉結點 R15
##== 繪製決策樹
plot(iris.tree);     text(iris.tree)  #-- 6個葉結點, 其餘均為決策結點, 數據分裂為二元劃分
##== 從決策樹到AI的規則庫(rulebase)
#    -- node 2 ---> R2:  IF (Petal.Length<2.45) THEN Species=setosa
#    -- node 25---> R25: IF (Petal.Length>2.45)&(Petal.Width<1.75)&(Petal.Length<4.95)&(Sepal.Length>5.15)
#                                               THEN Species=versicolor
#    -- node 15---> R15: IF (Petal.Length>4.95)&(Petal.Width<1.75) THEN Species=virginica
#    -- nodes FR24,FR13,FR14 為未確定規則(uncertain rules,為粗糙集rough set中的定義)，如:
#                  FR13: IF (Petal.Length>4.95)&(Petal.Width>1.75) THEN Species=virginica (prob=0.66667)
##== 模型的預測階段: 由所求取的模型(M),對新的數據(u_new)進行輸出結果預測 y_predict = predict(M,u_new)
Species.new.tree3 = predict(iris.tree, newdata=iris,level=0.95,interval="confidence")
##== iris圖上的決策屬性分割: iris--iris.tree
plot.iris(Petal.Width, Petal.Length, "Petal.Width", "Petal.Length")
abline(h=2.45,col="purple");  segments(1.75,2.45,1.75,7,col="pink");  segments(0,4.95,1.75,4.95,col="red");
#    -- 可以看出，每一個決策結點，為一個直角分割(rectangular partition), 如:
#       -- 決策結點 2與3 的分支是由圖上的 紫色橫線(Petal.Length=2.45) 來進行數據分裂
#       -- 決策結點 6與7 的分支是由圖上的 粉色直線(Petal.Width=1.75) 來進行數據分裂
#       -- 決策結點14與15的分支是由圖上的 紅色橫線(Petal.Length=4.95) 來進行數據分裂
##== 混淆矩陣(confusion matrix): 比較 原輸出結果 與 預測輸出結果 的差別
Species.new.tree = apply(Species.new.tree3, 1, which.max); Species.new.tree[41:60] #-- 將150*3的陣列 轉成 150*1 的向量
# 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60
#  1  1  1  1  1  1  1  1  1  1  2  2  2  2  2  2  2  2  2  2
table( Species.new.tree, Species)
#####===== (1C) (HUT05:1C-1D,HUT07-3A) iris 數據 =====#####
dim(iris);   head(iris,2)
#   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
# 1          5.1         3.5          1.4         0.2  setosa
# 2          4.9         3.0          1.4         0.2  setosa
##== 數據的筆數為150筆，共有五個欄位(前四個單位為公分)：
#--     花萼長度(Sepal Length),花萼寬度(Sepal Width),花瓣長度(Petal Length),花瓣寬度(Petal Width),
#--     類別(Class)：三個品種Setosa，Versicolor和Virginica
attach(iris)   ##== (1) 設定數據框為iris, 可以精簡以下的變量表示
##== 鳶尾花類別分布圖
plot.iris <- function(xx,yy,xxlab,yylab) {
plot(NULL,xlim=range(xx),ylim=range(yy),main="classified scatter plot of iris data",xlab=xxlab,ylab=yylab)
points(xx[Species=="setosa"],yy[Species=="setosa"],pch=1,col="blue")
points(xx[Species=="virginica"],yy[Species== "virginica"],pch=2,col="green")
points(xx[Species=="versicolor"],yy[Species== "versicolor"],pch=3,col="purple")
legend("topleft",legend=c("setosa","virginica","versicolor"), bty="n",col=c("blue","green","purple"),x.intersp=0.5, y.intersp=0.5,pch=c(1,2,3))
}
plot.iris(Petal.Width, Petal.Length, "Petal.Width", "Petal.Length")
#####=====*(2B) iris例說明決策樹tree操作程序及其中觀念 [殷,7.2, 7.9.2] =====#####
library(tree)
##== 模型的訓練階段: 由輸入數據(u)輸出數據(y) 求取模型 M = tree(y~u)
iris.tree = tree(Species ~ .,data=iris);    iris.tree   #-- * denotes terminal node(葉結點或終結點)
#   3) Petal.Length > 2.45 100 138.600 versicolor ( 0.00000 0.50000 0.50000 )
#     6) Petal.Width < 1.75 54  33.320 versicolor ( 0.00000 0.90741 0.09259 )
#      12) Petal.Length < 4.95 48   9.721 versicolor ( 0.00000 0.97917 0.02083 )
#        24) Sepal.Length < 5.15 5   5.004 versicolor ( 0.00000 0.80000 0.20000 ) * 葉結點 FR24
#        25) Sepal.Length > 5.15 43   0.000 versicolor ( 0.00000 1.00000 0.00000 ) * 葉結點 R25
#      13) Petal.Length > 4.95 6   7.638 virginica ( 0.00000 0.33333 0.66667 ) * 葉結點 FR13
#     7) Petal.Width > 1.75 46   9.635 virginica ( 0.00000 0.02174 0.97826 )
#      14) Petal.Length < 4.95 6   5.407 virginica ( 0.00000 0.16667 0.83333 ) * 葉結點 FR14
#      15) Petal.Length > 4.95 40   0.000 virginica ( 0.00000 0.00000 1.00000 ) * 葉結點 R15
##== 繪製決策樹
plot(iris.tree);     text(iris.tree)  #-- 6個葉結點, 其餘均為決策結點, 數據分裂為二元劃分
##== 從決策樹到AI的規則庫(rulebase)
#    -- node 2 ---> R2:  IF (Petal.Length<2.45) THEN Species=setosa
#    -- node 25---> R25: IF (Petal.Length>2.45)&(Petal.Width<1.75)&(Petal.Length<4.95)&(Sepal.Length>5.15)
#                                               THEN Species=versicolor
#    -- node 15---> R15: IF (Petal.Length>4.95)&(Petal.Width<1.75) THEN Species=virginica
#    -- nodes FR24,FR13,FR14 為未確定規則(uncertain rules,為粗糙集rough set中的定義)，如:
#                  FR13: IF (Petal.Length>4.95)&(Petal.Width>1.75) THEN Species=virginica (prob=0.66667)
##== 模型的預測階段: 由所求取的模型(M),對新的數據(u_new)進行輸出結果預測 y_predict = predict(M,u_new)
Species.new.tree3 = predict(iris.tree, newdata=iris,level=0.95,interval="confidence")
##== iris圖上的決策屬性分割: iris--iris.tree
plot.iris(Petal.Width, Petal.Length, "Petal.Width", "Petal.Length")
abline(h=2.45,col="purple");  segments(1.75,2.45,1.75,7,col="pink");  segments(0,4.95,1.75,4.95,col="red");
#    -- 可以看出，每一個決策結點，為一個直角分割(rectangular partition), 如:
#       -- 決策結點 2與3 的分支是由圖上的 紫色橫線(Petal.Length=2.45) 來進行數據分裂
#       -- 決策結點 6與7 的分支是由圖上的 粉色直線(Petal.Width=1.75) 來進行數據分裂
#       -- 決策結點14與15的分支是由圖上的 紅色橫線(Petal.Length=4.95) 來進行數據分裂
##== 混淆矩陣(confusion matrix): 比較 原輸出結果 與 預測輸出結果 的差別
Species.new.tree = apply(Species.new.tree3, 1, which.max); Species.new.tree[41:60] #-- 將150*3的陣列 轉成 150*1 的向量
# 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60
#  1  1  1  1  1  1  1  1  1  1  2  2  2  2  2  2  2  2  2  2
table( Species.new.tree, Species)
#####===== (2C) iris例說明決策樹rpart操作程序 =====#####
library(rpart);   library(rpart.plot)
##== 訓練模型
iris.rpart = rpart(Species ~. , data=iris);   print(iris.rpart)
# 1) root 150 100 setosa (0.33333333 0.33333333 0.33333333)
#   2) Petal.Length< 2.45 50   0 setosa (1.00000000 0.00000000 0.00000000) *
#   3) Petal.Length>=2.45 100  50 versicolor (0.00000000 0.50000000 0.50000000)
#     6) Petal.Width< 1.75 54   5 versicolor (0.00000000 0.90740741 0.09259259) *
#     7) Petal.Width>=1.75 46   1 virginica (0.00000000 0.02173913 0.97826087) *
##== 繪出決策樹
rpart.plot(iris.rpart)
##== 預測值
Species.new.rpart3 = predict(iris.rpartA, newdata=iris);   Species.new.rpart3
##== 預測值
Species.new.rpart3 = predict(iris.rpartA, newdata=iris);   Species.new.rpart3
##== 預測值
Species.new.rpart3 = predict(iris.rpart, newdata=XX);   Species.new.rpart3
#=========R-tech6================
setwd("/Users/juck30808/Documents/Github/USC_R_Git/2.R-tech/data")
#X <- as.data.frame(read_excel('/Users/juck30808/Documents/Github/USC_R_Git/2.R-tech/data/googleplaystore4_(7000).xlsX'))
X = read.csv("googleplaystore4_(7000).csv");   dim(X);   head(X,2)
##== App 收費 type 分布圖
dim(X); head(X,2)
attach(X)   ##== (1) 設定數據框為X, 可以精簡以下的變量表示
plot.app <- function(xx,yy,xxlab,yylab) {
plot(NULL,xlim=range(xx),ylim=range(yy),
main="classified scatter plot of iris data",
xlab=xxlab,ylab=yylab)
points(xx[Type=="Free"],yy[Type=="Free"],pch=1,col="blue")
points(xx[Type=="Paid"],yy[Type== "Paid"],pch=2,col="red")
legend("topleft",
legend=c("Free","Paid"), bty="n",
col=c("blue","red"),
x.intersp=0.5, y.intersp=0.5,
pch=c(1,2))
}
Reviews.log =log10(Reviews)
Rating.log = log10(Rating)
plot.app(Rating,Reviews.log, "Rating", "Reviews")  #收費不一定比較好
#####=====*(2B) iris例說明決策樹tree操作程序及其中觀念 [殷,7.2, 7.9.2] =====#####
library(tree);library(rpart)
X$Reviews.log =log10(X$Reviews)
XX = as.data.frame(X[c(1:10,170:220),c("Rating","Reviews.log","Type")]);   dim(XX);   head(XX,2)
#因變數   #自變數
# iris.tree = tree(Type ~ Rating + Reviews.log, data=XX);    iris.tree
# iris.tree = tree(Type ~ Rating + Reviews, data=X[c(1:10,170:220),]);    iris.tree
# iris.rpart = rpart(Type ~ Rating + Reviews.log, data=XX);    iris.rpart
iris.rpart = rpart(Type ~ Rating + Reviews.log, data=XX,
control=rpart.control(minsplit=6, maxdepth=4));    iris.rpart
##== 繪製決策樹
plot(iris.rpart);     text(iris.rpart)  #-- 6個葉結點, 其餘均為決策結點, 數據分裂為二元劃分
Type = predict(iris.rpart, newdata=X,level=0.95,interval="confidence")
##== 訓練模型
library(rpart);   library(rpart.plot)
##== 訓練模型
iris.rpart = rpart(Type ~ Rating + Reviews.log, data=XX,
control=rpart.control(minsplit=6, maxdepth=4));
print(iris.rpart); rpart.plot(iris.rpart)
##== 預測值
Species.new.rpart3 = predict(iris.rpart, newdata=XX);   Species.new.rpart3
Species.new.rpart = apply(Species.new.rpart3, 1, which.max)
##== 預測值
Type.nr3 = predict(iris.rpart, newdata=XX);   Species.new.rpart3
Type.nr = apply(Species.new.rpart3, 1, which.max)
##== 混淆矩陣
table( Type.nr3, Type.nr)
##== 預測值
Species.new.rpart3 = predict(iris.rpart, newdata=XX);   Species.new.rpart3
Species.new.rpart = apply(Species.new.rpart3, 1, which.max)
##== 混淆矩陣
table( Species.new.rpart, Species)
##== 預測值
Type.nr3 = predict(iris.rpart, newdata=XX);   Type.nr3
Type.nr = apply(Type.nr3, 1, which.max)
##== 混淆矩陣
table( Species.new.rpart, Species)
##== 混淆矩陣
table( Type.nr, Type)
Type = predict(iris.rpart, newdata=X,level=0.95,interval="confidence")
##== 訓練模型
iris.rpart = rpart(Type ~ Rating + Reviews.log, data=XX,
control=rpart.control(minsplit=6, maxdepth=4));
print(iris.rpart); rpart.plot(iris.rpart)
##== 預測值
Type.nr3 = predict(iris.rpart, newdata=XX);   Type.nr3
Type.nr = apply(Type.nr3, 1, which.max)
##== 混淆矩陣
table( Type.nr, Type)
Type.nr = apply(Type.nr3, 1, which.max) Type.nr
##== 預測值
Type.nr3 = predict(iris.rpart, newdata=XX);   Type.nr3
Type.nr = apply(Type.nr3, 1, which.max) Type.nr
#####===== (2C) iris例說明決策樹rpart操作程序 =====#####
library(rpart);   library(rpart.plot)
##== 訓練模型
iris.rpart = rpart(Species ~. , data=iris);   print(iris.rpart)
# 1) root 150 100 setosa (0.33333333 0.33333333 0.33333333)
#   2) Petal.Length< 2.45 50   0 setosa (1.00000000 0.00000000 0.00000000) *
#   3) Petal.Length>=2.45 100  50 versicolor (0.00000000 0.50000000 0.50000000)
#     6) Petal.Width< 1.75 54   5 versicolor (0.00000000 0.90740741 0.09259259) *
#     7) Petal.Width>=1.75 46   1 virginica (0.00000000 0.02173913 0.97826087) *
##== 繪出決策樹
rpart.plot(iris.rpart)
##== 預測值
Species.new.rpart3 = predict(iris.rpartA, newdata=iris);   Species.new.rpart3
##== 預測值
Species.new.rpart3 = predict(iris.rpart, newdata=iris);   Species.new.rpart3
Species.new.rpart = apply(Species.new.rpart3, 1, which.max)
##== 預測值
Species.new.rpart3 = predict(iris.rpart, newdata=iris);   Species.new.rpart3
Species.new.rpart = apply(Species.new.rpart3, 1, which.max) Species.new.rpart
##== 混淆矩陣
table( Species.new.rpart, Species)
##== 預測值
Species.new.rpart3 = predict(iris.rpart, newdata=iris);   Species.new.rpart3
Species.new.rpart = apply(Species.new.rpart3, 1, which.max) Species.new.rpart
#####===== (1C) (HUT05:1C-1D,HUT07-3A) iris 數據 =====#####
dim(iris);   head(iris,2)
#   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
# 1          5.1         3.5          1.4         0.2  setosa
# 2          4.9         3.0          1.4         0.2  setosa
##== 數據的筆數為150筆，共有五個欄位(前四個單位為公分)：
#--     花萼長度(Sepal Length),花萼寬度(Sepal Width),花瓣長度(Petal Length),花瓣寬度(Petal Width),
#--     類別(Class)：三個品種Setosa，Versicolor和Virginica
attach(iris)   ##== (1) 設定數據框為iris, 可以精簡以下的變量表示
##== 鳶尾花類別分布圖
plot.iris <- function(xx,yy,xxlab,yylab) {
plot(NULL,xlim=range(xx),ylim=range(yy),main="classified scatter plot of iris data",xlab=xxlab,ylab=yylab)
points(xx[Species=="setosa"],yy[Species=="setosa"],pch=1,col="blue")
points(xx[Species=="virginica"],yy[Species== "virginica"],pch=2,col="green")
points(xx[Species=="versicolor"],yy[Species== "versicolor"],pch=3,col="purple")
legend("topleft",legend=c("setosa","virginica","versicolor"), bty="n",col=c("blue","green","purple"),x.intersp=0.5, y.intersp=0.5,pch=c(1,2,3))
}
plot.iris(Petal.Width, Petal.Length, "Petal.Width", "Petal.Length")
#####=====*(2B) iris例說明決策樹tree操作程序及其中觀念 [殷,7.2, 7.9.2] =====#####
library(tree)
##== 模型的訓練階段: 由輸入數據(u)輸出數據(y) 求取模型 M = tree(y~u)
iris.tree = tree(Species ~ .,data=iris);    iris.tree   #-- * denotes terminal node(葉結點或終結點)
#   3) Petal.Length > 2.45 100 138.600 versicolor ( 0.00000 0.50000 0.50000 )
#     6) Petal.Width < 1.75 54  33.320 versicolor ( 0.00000 0.90741 0.09259 )
#      12) Petal.Length < 4.95 48   9.721 versicolor ( 0.00000 0.97917 0.02083 )
#        24) Sepal.Length < 5.15 5   5.004 versicolor ( 0.00000 0.80000 0.20000 ) * 葉結點 FR24
#        25) Sepal.Length > 5.15 43   0.000 versicolor ( 0.00000 1.00000 0.00000 ) * 葉結點 R25
#      13) Petal.Length > 4.95 6   7.638 virginica ( 0.00000 0.33333 0.66667 ) * 葉結點 FR13
#     7) Petal.Width > 1.75 46   9.635 virginica ( 0.00000 0.02174 0.97826 )
#      14) Petal.Length < 4.95 6   5.407 virginica ( 0.00000 0.16667 0.83333 ) * 葉結點 FR14
#      15) Petal.Length > 4.95 40   0.000 virginica ( 0.00000 0.00000 1.00000 ) * 葉結點 R15
##== 繪製決策樹
plot(iris.tree);     text(iris.tree)  #-- 6個葉結點, 其餘均為決策結點, 數據分裂為二元劃分
##== 從決策樹到AI的規則庫(rulebase)
#    -- node 2 ---> R2:  IF (Petal.Length<2.45) THEN Species=setosa
#    -- node 25---> R25: IF (Petal.Length>2.45)&(Petal.Width<1.75)&(Petal.Length<4.95)&(Sepal.Length>5.15)
#                                               THEN Species=versicolor
#    -- node 15---> R15: IF (Petal.Length>4.95)&(Petal.Width<1.75) THEN Species=virginica
#    -- nodes FR24,FR13,FR14 為未確定規則(uncertain rules,為粗糙集rough set中的定義)，如:
#                  FR13: IF (Petal.Length>4.95)&(Petal.Width>1.75) THEN Species=virginica (prob=0.66667)
##== 模型的預測階段: 由所求取的模型(M),對新的數據(u_new)進行輸出結果預測 y_predict = predict(M,u_new)
Species.new.tree3 = predict(iris.tree, newdata=iris,level=0.95,interval="confidence")
##== iris圖上的決策屬性分割: iris--iris.tree
plot.iris(Petal.Width, Petal.Length, "Petal.Width", "Petal.Length")
abline(h=2.45,col="purple");  segments(1.75,2.45,1.75,7,col="pink");  segments(0,4.95,1.75,4.95,col="red");
#    -- 可以看出，每一個決策結點，為一個直角分割(rectangular partition), 如:
#       -- 決策結點 2與3 的分支是由圖上的 紫色橫線(Petal.Length=2.45) 來進行數據分裂
#       -- 決策結點 6與7 的分支是由圖上的 粉色直線(Petal.Width=1.75) 來進行數據分裂
#       -- 決策結點14與15的分支是由圖上的 紅色橫線(Petal.Length=4.95) 來進行數據分裂
##== 混淆矩陣(confusion matrix): 比較 原輸出結果 與 預測輸出結果 的差別
Species.new.tree = apply(Species.new.tree3, 1, which.max); Species.new.tree[41:60] #-- 將150*3的陣列 轉成 150*1 的向量
# 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60
#  1  1  1  1  1  1  1  1  1  1  2  2  2  2  2  2  2  2  2  2
table( Species.new.tree, Species)
#####===== (2C) iris例說明決策樹rpart操作程序 =====#####
library(rpart);   library(rpart.plot)
##== 訓練模型
iris.rpart = rpart(Species ~. , data=iris);   print(iris.rpart)
# 1) root 150 100 setosa (0.33333333 0.33333333 0.33333333)
#   2) Petal.Length< 2.45 50   0 setosa (1.00000000 0.00000000 0.00000000) *
#   3) Petal.Length>=2.45 100  50 versicolor (0.00000000 0.50000000 0.50000000)
#     6) Petal.Width< 1.75 54   5 versicolor (0.00000000 0.90740741 0.09259259) *
#     7) Petal.Width>=1.75 46   1 virginica (0.00000000 0.02173913 0.97826087) *
##== 繪出決策樹
rpart.plot(iris.rpart)
##== 預測值
Species.new.rpart3 = predict(iris.rpartA, newdata=iris);   Species.new.rpart3
Species.new.rpart = apply(Species.new.rpart3, 1, which.max)
# 1) root 150 100 setosa (0.33333333 0.33333333 0.33333333)
#   2) Petal.Length< 2.45 50   0 setosa (1.00000000 0.00000000 0.00000000) *
#   3) Petal.Length>=2.45 100  50 versicolor (0.00000000 0.50000000 0.50000000)
#     6) Petal.Width< 1.75 54   5 versicolor (0.00000000 0.90740741 0.09259259) *
#     7) Petal.Width>=1.75 46   1 virginica (0.00000000 0.02173913 0.97826087) *
##== 繪出決策樹
rpart.plot(iris.rpart)
##== 預測值
Species.new.rpart3 = predict(iris.rpart, newdata=iris);   Species.new.rpart3
Species.new.rpart = apply(Species.new.rpart3, 1, which.max)
Species.new.rpart = apply(Species.new.rpart3, 1, which.max)  Species.new.rpart
##== 預測值
Species.new.rpart3 = predict(iris.rpart, newdata=iris);   Species.new.rpart3
#Species.new.rpart = apply(Species.new.rpart3, 1, which.max) Species.new.rpart
##== 混淆矩陣
table( Species.new.rpart3, Type)
#                   Species
# Species.new.rpart setosa versicolor virginica
#                 1     50          0         0
#                 2      0         47         1
#                 3      0          3        49
##== rpart決策樹參數
#    -- minsplit：每一個節點的最少數據(data)數
#    -- minbucket：終端節點(terminal node)的最少數據(data)數
#    -- maxdepth：決策樹的深度
##== 加上參數求得的決策樹
iris.rpartA = rpart(Species ~. , data=iris, control=rpart.control(minsplit=6, maxdepth=4))
#因變數   #自變數
# iris.tree = tree(Type ~ Rating + Reviews.log, data=XX);    iris.tree
# iris.tree = tree(Type ~ Rating + Reviews, data=X[c(1:10,170:220),]);    iris.tree
# iris.rpart = rpart(Type ~ Rating + Reviews.log, data=XX);    iris.rpart
iris.rpart = rpart(Type ~ Rating + Reviews.log, data=XX,
control=rpart.control(minsplit=6, maxdepth=4));    iris.rpart
##== 繪製決策樹
plot(iris.rpart);     text(iris.rpart)  #-- 6個葉結點, 其餘均為決策結點, 數據分裂為二元劃分
Type = predict(iris.rpart, newdata=X,level=0.95,interval="confidence")
##== 訓練模型
iris.rpart = rpart(Type ~ Rating + Reviews.log, data=XX,
control=rpart.control(minsplit=6, maxdepth=4));
print(iris.rpart); rpart.plot(iris.rpart)
